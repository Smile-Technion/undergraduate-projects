# -*- coding: utf-8 -*-
"""DeepLearning_InverseKinematics_Net.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ApaOZuyQetX4Gze0wUFZ6tqGwoOahEKo
"""

from numpy import loadtxt
from keras.models import Sequential
from keras.layers import Dense
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from keras.callbacks import ModelCheckpoint
from keras.layers import Dropout
# load the dataset
# split into input (X) and output (y) variables
X = pd.read_csv('inputData.csv')
y = pd.read_csv('outputData.csv')
# define the keras model install tensorflow
print(X.shape)
print(y.shape)
model = Sequential()
model.add(Dense(12, input_dim=6, activation='relu'))
# model.add(Dropout(0.3))
# model.add(Dense(25, activation='relu'))
model.add(Dense(6, activation='linear'))
model.summary()
# compile the keras model
model.compile(loss=tf.keras.losses.MSE, optimizer='adam', metrics=['accuracy'])
# fit the keras model on the dataset
history=model.fit(X, y,validation_split=0.3 ,epochs=30, batch_size=100)
# evaluate the keras model
_, accuracy = model.evaluate(X, y)
print('Accuracy: %.2f' % (accuracy*100))
# checkpoint
filepath="weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# predictions =model.predict(Xnew)(X)
# predictions =model.predict(X.iloc[0:1])
# print(predictions)
model_json = model.to_json()
with open("model.json", "w") as json_file:
    json_file.write(model_json)
# serialize weights to HDF5
model.save_weights("model.h5")
print("Saved model to disk")
 
# later...
model.save('my_model.h5')
# model.add(Dropout(0.5))

# # load json and create model
# json_file = open('model.json', 'r')
# loaded_model_json = json_file.read()
# json_file.close()
# loaded_model = model_from_json(loaded_model_json)
# # load weights into new model
# loaded_model.load_weights("model.h5")
# print("Loaded model from disk")
 
# # evaluate loaded model on test data
# loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
# score = loaded_model.evaluate(X, Y, verbose=0)
# print("%s: %.2f%%" % (loaded_model.metrics_names[1], score[1]*100))

# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()





##load model
# import pandas as pd
# import tensorflow as tf
# import numpy as np
# import gym
# X = pd.read_csv('inputData_1.csv')
# y = pd.read_csv('outputdata_1.csv')
# new_model = tf.keras.models.load_model('my_model.h5')
# # Check its architecture
# new_model.summary()
# # Evaluate the restored model
# loss, acc = new_model.evaluate(X,  y, verbose=2)
# print('Restored model, accuracy: {:5.2f}%'.format(100*acc))
# # new_model.predict(X.iloc[0])
# print(new_model.predict(X.iloc[0:1]))

# from sklearn.datasets import make_regression
# from sklearn.preprocessing import StandardScaler
# from keras.models import Sequential
# from keras.layers import Dense
# from keras.optimizers import SGD
# from matplotlib import pyplot
# import numpy as np
# X = pd.read_csv('inputData.csv')
# y = pd.read_csv('outputData.csv')
# n_train =int( np.round(199999*0.7))
# trainX, testX = X.iloc[:n_train, :], X.iloc[n_train:, :]
# trainy, testy = y.iloc[:n_train], y.iloc[n_train:]
# # define model
# model = Sequential()
# model.add(Dense(25, input_dim=6, activation='relu', kernel_initializer='he_uniform'))
# model.add(Dense(6, activation='linear'))
# opt = SGD(lr=0.01, momentum=0.9)
# model.compile(loss='mean_absolute_error', optimizer=opt, metrics=['mse'])
# # fit model
# history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=1, verbose=0)
# # evaluate the model
# _, train_mse = model.evaluate(trainX, trainy, verbose=0)
# _, test_mse = model.evaluate(testX, testy, verbose=0)
# print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))
# # plot loss during training
# pyplot.subplot(211)
# pyplot.title('Loss')
# pyplot.plot(history.history['loss'], label='train')
# pyplot.plot(history.history['val_loss'], label='test')
# pyplot.legend()
# # plot mse during training
# pyplot.subplot(212)
# # pyplot.title('Mean Squared Error')
# predictions = model.predict_classes(X.iloc[0:1])
# print(predictions)

# # pyplot.plot(history.history['mean_squared_error'], label='train')
# # pyplot.plot(history.history['val_mean_squared_error'], label='test')
# # pyplot.legend()
# # pyplot.show()

predictions =model.predict(X.iloc[0:1])
print(predictions)

